---
title: "Hierachiacal clustering"
author: "Andrew Witherspoon"
date: "9/30/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

dir <- "/Users/andrew/Course4Week3"
setwd(dir)
```

##Hiarchiacal clustering - example

First we will plot 12 point, and label them:
```{r}
set.seed(1234)
par(mar = c(0,0,0,0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1,2,1), each = 4), sd = 0.2)
plot(x, y, col = "blue", pch = 19, cex = 2)
text(x + 0.05, y  + 0.05, labels = as.character(1:12))
```

As we can see, 1,2,3,4 look like a cluster, so do 5,6,7 - and also 9,10,11,12.

Now we will run a Heirarchical clustering algorithm.  The first thing we need to do is calculate the distance between each pair of points:
```{r}
dataFrame <- data.frame(x = x, y = y) #create a dataframe of coordinantes (x = x coordiates, y = y coordinates)
dist(dataFrame) #dist() function will give distance between each pair of points
```

Now we will want to cluster the two points that are closest together, and call them a new point.  Then (with the replacement point replacing the first two), find the new closest points.

We can do this with the R function hclust(), which will provide a dendogram of the order the points were clustered all the way to a single point:
```{r}
dataFrame <- data.frame(x = x, y = y) #create a dataframe of coordinantes (x = x coordiates, y = y coordinates)
distxy <- dist(dataFrame) #dist() function will give distance between each pair of points
hClustering <- hclust(distxy)
plot(hClustering)
```

The dendogram does not show you how many clusters there are.  You have to decide a cut point.  At a cut point of 2, there are two clusters:
```{r}
plot(hClustering)
abline(h=2, lty=2)
```

At a cut point of 0.5 there are five clusters:
(2,3) (1,4) (9,10,11,12) (8) (5,6,7)
```{r}
plot(hClustering)
abline(h=0.5, lty=2)
```

It's natural to cut it at a level that appears to have the largest "height" distance between splits to be most meaningful (but there is no rule).  This would give us three clusters:
(1,2,3,4)  (9,10,11,12) (5,6,7,8)
```{r}
plot(hClustering)
abline(h=1, lty=2)
```

Here's a function we can use to make prettier dendograms, myplclust() from the "rafalib" package
```{r, message = FALSE}
library(rafalib)
myplclust(hClustering, lab = rep(1:3, each = 4), lab.col = rep(1:3, each = 4))
```


The heatmap() function is another way to visually cluster.  

It essentially runs a hierarchal cluster analysis on the rows of the table, and the observations of the table.  It's better for high demensional table data.  Here there are only two rows (x and y)
```{r}
dataFrame <- data.frame(x = x, y = y)
set.seed(143)
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]
heatmap(dataMatrix)
```